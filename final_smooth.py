# -*- coding: utf-8 -*-
"""final_smooth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y83uKxh5zVcTqVjkSfv8fEOGom1LqsLz
"""

# %% ------------------------------------------------------------
# Index of Consumer Sentiment smoothing â€” full Python replication
# Mirrors R/KFAS with MLE-estimated Q and time-varying H_t
# ------------------------------------------------------------

import numpy as np
import pandas as pd
from scipy.optimize import minimize
import matplotlib.pyplot as plt

# --- 1. URLs (GitHub raw content) ---
URL_PUBLISHED  = "https://raw.githubusercontent.com/oysteinm/smood/refs/heads/main/scaum-479.csv"
URL_INDIVIDUAL = "https://raw.githubusercontent.com/oysteinm/smood/refs/heads/main/AAk7MRJC.csv"

# --- 2. Load data from GitHub ---
ics_month = pd.read_csv(URL_PUBLISHED)
ics_month["date"] = pd.to_datetime(ics_month["yyyymm"], format="%Y%m", errors="coerce")
ics_month["ics_all"] = pd.to_numeric(ics_month["ics_all"], errors="coerce")
ics_data = ics_month[["date", "ics_all"]].rename(columns={"ics_all": "ics_published"})

ics_id = pd.read_csv(URL_INDIVIDUAL)
ics_id.columns = (
    ics_id.columns.str.strip()
    .str.lower()
    .str.replace(r"[^0-9a-zA-Z]+", "_", regex=True)
)
# Expect YYYYMM, id, ics, wt
ics_id["date"] = pd.to_datetime(ics_id["yyyymm"], format="%Y%m", errors="coerce")
ics_id["ics"] = pd.to_numeric(ics_id["ics"], errors="coerce")
ics_id["wt"]  = pd.to_numeric(ics_id["wt"], errors="coerce")

# --- 3. Compute weighted monthly stats ---
def monthly_stats(df):
    d = df.loc[np.isfinite(df["ics"]) & np.isfinite(df["wt"]) & (df["wt"] > 0)].copy()

    # group-wise weighted mean, n_eff, var
    out = []
    for date, g in d.groupby("date"):
        w = g["wt"].to_numpy()
        x = g["ics"].to_numpy()
        n = len(x)
        w_sum  = w.sum()
        w2_sum = (w**2).sum()
        n_eff  = (w_sum**2) / max(w2_sum, 1e-12)
        xhat   = (w * x).sum() / w_sum
        denom  = max(w_sum - (w2_sum / w_sum), 1e-8)
        s2_w   = (w * (x - xhat)**2).sum() / denom
        var_mean = s2_w / max(n_eff, 1e-8)
        se_mean  = np.sqrt(max(var_mean, 1e-12))
        out.append((date, n, n_eff, w_sum, w2_sum, xhat, s2_w, var_mean, se_mean))

    return pd.DataFrame(
        out, columns=["date", "n", "n_eff", "w_sum", "w2_sum",
                      "ics_hat", "s2_w", "var_mean", "se_mean"]
    ).sort_values("date").reset_index(drop=True)

ics_monthly = monthly_stats(ics_id)

# --- 4. Guard for H_t (measurement variance) ---
H = ics_monthly["var_mean"].to_numpy().astype(float)
y = ics_monthly["ics_hat"].to_numpy().astype(float)

good = np.isfinite(H) & (H > 0)
fallback = np.nanmedian(H[good]) if np.any(good) else (np.nanvar(y) * 0.05)
if not np.isfinite(fallback):
    fallback = np.nanvar(y) * 0.05
H[~good] = fallback
q05 = np.nanquantile(H, 0.05) if np.isfinite(np.nanquantile(H, 0.05)) else np.nan
floor_val = 0.1 * q05 if (np.isfinite(q05) and q05 > 0) else 0.1 * fallback
H = np.maximum(H, floor_val)

# --- 5. Kalman log-likelihood and Q estimation ---
def kalman_loglike(y, H, Q, P0=1e6):
    """Local level model log-likelihood (handles missing y)."""
    T = len(y)
    mu_pred = 0.0
    P_pred = P0 + Q
    ll = 0.0
    for t in range(T):
        yt = y[t]
        Ht = H[t]
        if np.isfinite(yt):
            S = P_pred + Ht
            S = np.maximum(S, 1e-8)
            v = yt - mu_pred
            ll -= 0.5 * (np.log(2*np.pi) + np.log(S) + (v*v)/S)
            K = P_pred / S
            mu_filt = mu_pred + K*v
            P_filt = (1 - K) * P_pred
        else:
            mu_filt = mu_pred
            P_filt = P_pred
        mu_pred = mu_filt
        P_pred  = P_filt + Q
    return ll

def estimate_Q_mle(y, H, q0=None):
    if q0 is None:
        q0 = np.nanmean(H) * 0.05
        if not np.isfinite(q0) or q0 <= 0:
            q0 = np.nanvar(y) * 0.05
    def nll(logQ):
        Q = np.exp(logQ)
        return -kalman_loglike(y, H, Q)
    res = minimize(nll, x0=np.log(q0), method="L-BFGS-B")
    return float(np.exp(res.x)), res

Q_hat, res = estimate_Q_mle(y, H)
print(f"Estimated Q: {Q_hat:.6g}")

# --- 6. Kalman filter + RTS smoother ---
def kalman_filter_store(y, H, Q, P0=1e6):
    T = len(y)
    mu_pred = np.zeros(T)
    P_pred  = np.zeros(T)
    mu_filt = np.zeros(T)
    P_filt  = np.zeros(T)

    mu_pred[0] = 0.0
    P_pred[0]  = P0 + Q

    if np.isfinite(y[0]):
        S = P_pred[0] + H[0]
        S = max(S, 1e-8)
        K = P_pred[0] / S
        v = y[0] - mu_pred[0]
        mu_filt[0] = mu_pred[0] + K*v
        P_filt[0]  = (1 - K) * P_pred[0]
    else:
        mu_filt[0] = mu_pred[0]
        P_filt[0]  = P_pred[0]

    for t in range(1, T):
        mu_pred[t] = mu_filt[t-1]
        P_pred[t]  = P_filt[t-1] + Q
        if np.isfinite(y[t]):
            S = P_pred[t] + H[t]
            S = max(S, 1e-8)
            K = P_pred[t] / S
            v = y[t] - mu_pred[t]
            mu_filt[t] = mu_pred[t] + K*v
            P_filt[t]  = (1 - K) * P_pred[t]
        else:
            mu_filt[t] = mu_pred[t]
            P_filt[t]  = P_pred[t]
    return mu_pred, P_pred, mu_filt, P_filt

def rts_smoother(mu_pred, P_pred, mu_filt, P_filt, Q):
    T = len(mu_filt)
    mu_smooth = np.zeros(T)
    P_smooth  = np.zeros(T)
    mu_smooth[-1] = mu_filt[-1]
    P_smooth[-1]  = P_filt[-1]
    for t in range(T-2, -1, -1):
        denom = P_pred[t+1]
        denom = np.maximum(denom, 1e-8)
        J = P_filt[t] / denom
        mu_smooth[t] = mu_filt[t] + J*(mu_smooth[t+1] - mu_pred[t+1])
        P_smooth[t]  = P_filt[t] + (J**2)*(P_smooth[t+1] - P_pred[t+1])
    return mu_smooth, P_smooth

mu_pred, P_pred, mu_filt, P_filt = kalman_filter_store(y, H, Q_hat)
mu_smooth, P_smooth = rts_smoother(mu_pred, P_pred, mu_filt, P_filt, Q_hat)
se_smooth = np.sqrt(np.maximum(P_smooth, 0.0))

# --- 7. Combine results + join published index ---
out = ics_monthly.copy()
out["ics_sm"]    = mu_smooth
out["ics_sm_lo"] = mu_smooth - 1.96 * se_smooth
out["ics_sm_hi"] = mu_smooth + 1.96 * se_smooth
ics_cmp = out.merge(ics_data, on="date", how="left")

# --- 8. Plot Raw vs Smoothed (95% band) ---
plt.figure(figsize=(10, 5))
plt.fill_between(ics_cmp["date"], ics_cmp["ics_sm_lo"], ics_cmp["ics_sm_hi"],
                 color="lightblue", alpha=0.4, label="Smoothed 95% band")
plt.plot(ics_cmp["date"], ics_cmp["ics_hat"], color="red", label="Raw ICS", linewidth=1.2)
plt.plot(ics_cmp["date"], ics_cmp["ics_sm"], color="blue", label="Smoothed ICS", linewidth=1.5)
plt.title("Index of Consumer Sentiment: Raw vs Smoothed")
plt.ylabel("Index of Consumer Sentiment")
plt.legend()
plt.grid(alpha=0.25)
plt.tight_layout()
plt.show()

# --- 9. Diagnostics ---
valid = np.isfinite(ics_cmp["ics_hat"]) & np.isfinite(ics_cmp["ics_sm"])
corr_raw_sm = np.corrcoef(ics_cmp.loc[valid, "ics_hat"], ics_cmp.loc[valid, "ics_sm"])[0,1]
corr_round1 = np.corrcoef(np.round(ics_cmp.loc[valid, "ics_hat"],1),
                          np.round(ics_cmp.loc[valid, "ics_sm"],1))[0,1]
print(f"Correlation raw-smoothed: {corr_raw_sm:.6f}")
print(f"Correlation (1-decimal): {corr_round1:.6f}")