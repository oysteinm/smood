{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance-Aware Smoothing for Survey KPIs\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a principled approach to smoothing time series survey data that accounts for varying sample sizes and measurement uncertainty across time periods.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Weekly or monthly survey KPIs exhibit noise because each period has finite (and often varying) sample size $n_t$. This sampling error:\n",
    "- Adds variance without adding information\n",
    "- Creates noisy dashboards and unstable trends\n",
    "- Makes period-to-period comparisons difficult\n",
    "\n",
    "**Key insight:** Not all observations are equally reliable. A month with $n=50$ respondents should be smoothed more than a month with $n=500$.\n",
    "\n",
    "### Our Approach\n",
    "\n",
    "We use **Kalman smoothing** with period-specific measurement variance to:\n",
    "1. Estimate measurement uncertainty for each period based on effective sample size\n",
    "2. Apply optimal smoothing that adapts to data quality\n",
    "3. Provide uncertainty bands for the smoothed estimates\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### Step 1: Weighted Mean and Measurement Variance\n",
    "\n",
    "For each time period $t$, from individual microdata $(y_{it}, w_{it})$:\n",
    "\n",
    "**Weighted mean:**\n",
    "$$\\hat{y}_t = \\frac{\\sum_i w_{it} y_{it}}{\\sum_i w_{it}}$$\n",
    "\n",
    "**Effective sample size** (accounts for unequal weights):\n",
    "$$n_{\\text{eff},t} = \\frac{(\\sum_i w_{it})^2}{\\sum_i w_{it}^2}$$\n",
    "\n",
    "**Weighted sample variance** (Bessel-corrected):\n",
    "$$s_{w,t}^2 = \\frac{\\sum_i w_{it}(y_{it} - \\hat{y}_t)^2}{\\sum_i w_{it} - \\frac{\\sum_i w_{it}^2}{\\sum_i w_{it}}}$$\n",
    "\n",
    "**Measurement variance of the mean:**\n",
    "$$H_t = \\text{Var}(\\hat{y}_t) \\approx \\frac{s_{w,t}^2}{n_{\\text{eff},t}}$$\n",
    "\n",
    "This gives us period-specific uncertainty: small $n_{\\text{eff},t}$ → large $H_t$ → more smoothing.\n",
    "\n",
    "#### Step 2: State Space Model (Kalman Filter/Smoother)\n",
    "\n",
    "We model the true KPI $\\mu_t$ as a slowly evolving local level:\n",
    "\n",
    "**Observation equation:**\n",
    "$$y_t = \\mu_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, H_t)$$\n",
    "\n",
    "**State equation:**\n",
    "$$\\mu_t = \\mu_{t-1} + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, Q)$$\n",
    "\n",
    "Where:\n",
    "- $H_t$: measurement variance (from survey sampling, varies by period)\n",
    "- $Q$: process variance (how much the true signal can change month-to-month)\n",
    "\n",
    "#### Step 3: Signal-to-Noise Ratio (SNR)\n",
    "\n",
    "To avoid identifiability issues when both $Q$ and $H$ are unknown, we fix their ratio:\n",
    "\n",
    "$$\\text{SNR} = \\frac{Q}{Q + \\bar{H}}$$\n",
    "\n",
    "Where $\\bar{H}$ is the average measurement variance. We then estimate only the overall scale $\\sigma^2$:\n",
    "- $Q = \\text{SNR} \\cdot \\sigma^2$\n",
    "- $H_t = (1 - \\text{SNR}) \\cdot \\sigma^2 \\cdot \\frac{H_t}{\\bar{H}}$ (scaled by relative measurement variance)\n",
    "\n",
    "**Interpretation:**\n",
    "- SNR = 0.01 → Very smooth (1% signal, 99% noise)\n",
    "- SNR = 0.05 → Moderate smoothing (5% signal, 95% noise)\n",
    "- SNR = 0.10 → Light smoothing (10% signal, 90% noise)\n",
    "\n",
    "### Data: University of Michigan Index of Consumer Sentiment\n",
    "\n",
    "We'll use the monthly ICS data which includes:\n",
    "- Individual response microdata with survey weights\n",
    "- Published aggregate index values\n",
    "\n",
    "This is an ideal test case because we can:\n",
    "1. Calculate weighted means ourselves\n",
    "2. Estimate measurement variance from effective sample size\n",
    "3. Apply our smoothing procedure\n",
    "4. Compare to published values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from GitHub\n",
    "\n",
    "We'll load two datasets:\n",
    "1. **Individual microdata** (`AAk7MRJC.csv`): Individual survey responses with weights\n",
    "2. **Published aggregate** (`scaum-479.csv`): Official monthly ICS values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load individual microdata\n",
    "url_microdata = \"https://raw.githubusercontent.com/oysteinm/smood/refs/heads/main/AAk7MRJC.csv\"\n",
    "ics_microdata = pd.read_csv(url_microdata)\n",
    "\n",
    "# Convert YYYYMM to datetime\n",
    "ics_microdata['date'] = pd.to_datetime(ics_microdata['YYYYMM'], format='%Y%m')\n",
    "\n",
    "# Clean column names (lowercase)\n",
    "ics_microdata.columns = ics_microdata.columns.str.lower()\n",
    "\n",
    "print(f\"Microdata shape: {ics_microdata.shape}\")\n",
    "print(f\"\\nColumns: {list(ics_microdata.columns)}\")\n",
    "print(f\"\\nDate range: {ics_microdata['date'].min()} to {ics_microdata['date'].max()}\")\n",
    "ics_microdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load published aggregate data\n",
    "url_published = \"https://raw.githubusercontent.com/oysteinm/smood/refs/heads/main/scaum-479.csv\"\n",
    "ics_published = pd.read_csv(url_published)\n",
    "\n",
    "# Convert YYYYMM to datetime\n",
    "ics_published['date'] = pd.to_datetime(ics_published['YYYYMM'], format='%Y%m')\n",
    "\n",
    "# Select relevant columns\n",
    "ics_published = ics_published[['date', 'ICS_ALL']].rename(columns={'ICS_ALL': 'ics_published'})\n",
    "\n",
    "print(f\"Published data shape: {ics_published.shape}\")\n",
    "print(f\"Date range: {ics_published['date'].min()} to {ics_published['date'].max()}\")\n",
    "ics_published.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Monthly Weighted Means and Measurement Variance\n",
    "\n",
    "For each month, we compute:\n",
    "1. **Sample size** $n_t$ and **effective sample size** $n_{\\text{eff},t}$\n",
    "2. **Weighted mean** $\\hat{y}_t$\n",
    "3. **Weighted variance** $s_{w,t}^2$ (Bessel-corrected)\n",
    "4. **Measurement variance** $H_t = s_{w,t}^2 / n_{\\text{eff},t}$\n",
    "\n",
    "This captures period-specific uncertainty from sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_statistics(df, date_col='date', value_col='ics', weight_col='wt'):\n",
    "    \"\"\"\n",
    "    Calculate monthly weighted mean, effective sample size, and measurement variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Microdata with individual responses\n",
    "    date_col : str\n",
    "        Column name for date\n",
    "    value_col : str\n",
    "        Column name for the KPI value\n",
    "    weight_col : str\n",
    "        Column name for survey weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with monthly aggregated statistics\n",
    "    \"\"\"\n",
    "    # Filter valid observations\n",
    "    df_clean = df[\n",
    "        df[value_col].notna() & \n",
    "        df[weight_col].notna() & \n",
    "        (df[weight_col] > 0)\n",
    "    ].copy()\n",
    "    \n",
    "    # Group by month and calculate statistics\n",
    "    monthly_stats = []\n",
    "    \n",
    "    for date, group in df_clean.groupby(date_col):\n",
    "        y = group[value_col].values\n",
    "        w = group[weight_col].values\n",
    "        \n",
    "        # Sample size\n",
    "        n = len(y)\n",
    "        \n",
    "        # Weight sums\n",
    "        w_sum = np.sum(w)\n",
    "        w2_sum = np.sum(w**2)\n",
    "        \n",
    "        # Effective sample size\n",
    "        n_eff = (w_sum**2) / max(w2_sum, 1e-12)\n",
    "        \n",
    "        # Weighted mean\n",
    "        y_hat = np.sum(w * y) / w_sum\n",
    "        \n",
    "        # Bessel-corrected weighted variance denominator\n",
    "        denom = max(w_sum - (w2_sum / w_sum), 1e-8)\n",
    "        \n",
    "        # Weighted variance of units\n",
    "        s2_w = np.sum(w * (y - y_hat)**2) / denom\n",
    "        \n",
    "        # Variance of the mean (measurement variance)\n",
    "        var_mean = s2_w / max(n_eff, 1e-8)\n",
    "        \n",
    "        # Standard error of the mean\n",
    "        se_mean = np.sqrt(max(var_mean, 1e-12))\n",
    "        \n",
    "        # Design effect\n",
    "        deff = n / n_eff\n",
    "        \n",
    "        monthly_stats.append({\n",
    "            'date': date,\n",
    "            'n': n,\n",
    "            'n_eff': n_eff,\n",
    "            'deff': deff,\n",
    "            'ics_hat': y_hat,\n",
    "            's2_w': s2_w,\n",
    "            'var_mean': var_mean,\n",
    "            'se_mean': se_mean\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(monthly_stats)\n",
    "\n",
    "# Calculate monthly statistics\n",
    "ics_monthly = calculate_weighted_statistics(ics_microdata)\n",
    "\n",
    "print(f\"\\nMonthly data shape: {ics_monthly.shape}\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(ics_monthly[['n', 'n_eff', 'deff', 'ics_hat', 'se_mean']].describe())\n",
    "\n",
    "ics_monthly.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Against Published Data\n",
    "\n",
    "Let's check how closely our weighted means match the published ICS values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with published data\n",
    "ics_comparison = ics_monthly.merge(ics_published, on='date', how='left')\n",
    "\n",
    "# Calculate correlation (excluding missing published values)\n",
    "valid_comparison = ics_comparison.dropna(subset=['ics_published'])\n",
    "correlation = valid_comparison['ics_hat'].corr(valid_comparison['ics_published'])\n",
    "\n",
    "print(f\"Correlation between weighted mean and published ICS: {correlation:.6f}\")\n",
    "print(f\"\\nMean absolute difference: {np.mean(np.abs(valid_comparison['ics_hat'] - valid_comparison['ics_published'])):.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(ics_comparison['date'], ics_comparison['ics_hat'], \n",
    "        label='Weighted ICS (calculated)', linewidth=2, alpha=0.7)\n",
    "ax.plot(ics_comparison['date'], ics_comparison['ics_published'], \n",
    "        label='Published ICS', linewidth=2, alpha=0.7, linestyle='--')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Index of Consumer Sentiment', fontsize=12)\n",
    "ax.set_title('Comparison: Weighted ICS vs Published ICS', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Measurement Uncertainty\n",
    "\n",
    "Plot the raw monthly means with ±1.96 SE bands to show the varying uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Top panel: ICS with uncertainty bands\n",
    "ax1.fill_between(ics_monthly['date'], \n",
    "                  ics_monthly['ics_hat'] - 1.96 * ics_monthly['se_mean'],\n",
    "                  ics_monthly['ics_hat'] + 1.96 * ics_monthly['se_mean'],\n",
    "                  alpha=0.3, label='±1.96 SE (95% CI)')\n",
    "ax1.plot(ics_monthly['date'], ics_monthly['ics_hat'], \n",
    "         linewidth=2, label='Weighted Mean', color='darkblue')\n",
    "ax1.set_ylabel('ICS', fontsize=12)\n",
    "ax1.set_title('Index of Consumer Sentiment with Measurement Uncertainty', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom panel: Effective sample size over time\n",
    "ax2.plot(ics_monthly['date'], ics_monthly['n_eff'], \n",
    "         linewidth=2, color='green', label='Effective Sample Size')\n",
    "ax2.axhline(y=ics_monthly['n_eff'].median(), color='red', \n",
    "            linestyle='--', label=f'Median = {ics_monthly[\"n_eff\"].median():.0f}')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Effective n', fontsize=12)\n",
    "ax2.set_title('Effective Sample Size Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEffective sample size statistics:\")\n",
    "print(f\"  Median: {ics_monthly['n_eff'].median():.1f}\")\n",
    "print(f\"  Min: {ics_monthly['n_eff'].min():.1f}\")\n",
    "print(f\"  Max: {ics_monthly['n_eff'].max():.1f}\")\n",
    "print(f\"  Std: {ics_monthly['n_eff'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kalman Smoothing with Fixed Signal-to-Noise Ratio\n",
    "\n",
    "Now we apply the Kalman filter/smoother using the **local level model** with:\n",
    "- Time-varying measurement variance $H_t$ (from our calculations above)\n",
    "- Fixed signal-to-noise ratio to avoid identifiability issues\n",
    "\n",
    "### Implementation Note\n",
    "\n",
    "We'll use a custom implementation because `statsmodels` doesn't directly support time-varying measurement variance with the simple interface. We'll use the `UnobservedComponents` model with manual scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import solve\n",
    "\n",
    "class KalmanSmoother:\n",
    "    \"\"\"\n",
    "    Kalman filter and smoother for local level model with time-varying measurement variance.\n",
    "    \n",
    "    Model:\n",
    "        y_t = mu_t + eps_t,  eps_t ~ N(0, H_t)\n",
    "        mu_t = mu_{t-1} + eta_t,  eta_t ~ N(0, Q)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y, H, Q):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        y : array-like\n",
    "            Observed time series\n",
    "        H : array-like\n",
    "            Time-varying measurement variance (length T)\n",
    "        Q : float\n",
    "            Process variance (constant)\n",
    "        \"\"\"\n",
    "        self.y = np.asarray(y)\n",
    "        self.H = np.asarray(H)\n",
    "        self.Q = Q\n",
    "        self.T = len(y)\n",
    "        \n",
    "    def filter(self):\n",
    "        \"\"\"Run Kalman filter (forward pass)\"\"\"\n",
    "        # Initialize\n",
    "        self.mu_pred = np.zeros(self.T)\n",
    "        self.mu_filt = np.zeros(self.T)\n",
    "        self.P_pred = np.zeros(self.T)\n",
    "        self.P_filt = np.zeros(self.T)\n",
    "        \n",
    "        # Initial state (diffuse prior)\n",
    "        self.mu_pred[0] = self.y[0]\n",
    "        self.P_pred[0] = self.H[0] + self.Q\n",
    "        \n",
    "        # Forward pass\n",
    "        for t in range(self.T):\n",
    "            # Update (filter)\n",
    "            K = self.P_pred[t] / (self.P_pred[t] + self.H[t])  # Kalman gain\n",
    "            self.mu_filt[t] = self.mu_pred[t] + K * (self.y[t] - self.mu_pred[t])\n",
    "            self.P_filt[t] = (1 - K) * self.P_pred[t]\n",
    "            \n",
    "            # Predict (for next step)\n",
    "            if t < self.T - 1:\n",
    "                self.mu_pred[t + 1] = self.mu_filt[t]\n",
    "                self.P_pred[t + 1] = self.P_filt[t] + self.Q\n",
    "    \n",
    "    def smooth(self):\n",
    "        \"\"\"Run Kalman smoother (backward pass)\"\"\"\n",
    "        self.mu_smooth = np.zeros(self.T)\n",
    "        self.P_smooth = np.zeros(self.T)\n",
    "        \n",
    "        # Initialize at final time point\n",
    "        self.mu_smooth[-1] = self.mu_filt[-1]\n",
    "        self.P_smooth[-1] = self.P_filt[-1]\n",
    "        \n",
    "        # Backward pass\n",
    "        for t in range(self.T - 2, -1, -1):\n",
    "            J = self.P_filt[t] / max(self.P_pred[t + 1], 1e-10)  # Smoother gain\n",
    "            self.mu_smooth[t] = self.mu_filt[t] + J * (self.mu_smooth[t + 1] - self.mu_pred[t + 1])\n",
    "            self.P_smooth[t] = self.P_filt[t] + J**2 * (self.P_smooth[t + 1] - self.P_pred[t + 1])\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run both filter and smoother\"\"\"\n",
    "        self.filter()\n",
    "        self.smooth()\n",
    "        return self.mu_smooth, np.sqrt(self.P_smooth)\n",
    "\n",
    "\n",
    "def smooth_with_fixed_snr(y, H, snr=0.05):\n",
    "    \"\"\"\n",
    "    Smooth time series using Kalman filter with fixed signal-to-noise ratio.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Observed time series\n",
    "    H : array-like\n",
    "        Time-varying measurement variance\n",
    "    snr : float\n",
    "        Signal-to-noise ratio (0 < snr < 1)\n",
    "        Lower values = more smoothing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mu_smooth : array\n",
    "        Smoothed state estimates\n",
    "    se_smooth : array\n",
    "        Standard errors of smoothed estimates\n",
    "    Q : float\n",
    "        Estimated process variance\n",
    "    \"\"\"\n",
    "    # Estimate overall variance scale\n",
    "    H_mean = np.mean(H)\n",
    "    \n",
    "    # Set Q based on SNR\n",
    "    # Q / (Q + H_mean) = SNR\n",
    "    # Q = SNR * (Q + H_mean)\n",
    "    # Q = SNR * H_mean / (1 - SNR)\n",
    "    Q = snr * H_mean / (1 - snr)\n",
    "    \n",
    "    # Run Kalman smoother\n",
    "    smoother = KalmanSmoother(y, H, Q)\n",
    "    mu_smooth, se_smooth = smoother.run()\n",
    "    \n",
    "    return mu_smooth, se_smooth, Q\n",
    "\n",
    "print(\"Kalman smoother implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Kalman Smoothing with Different SNR Values\n",
    "\n",
    "We'll compare smoothing with different signal-to-noise ratios:\n",
    "- **SNR = 0.01**: Very smooth (99% noise)\n",
    "- **SNR = 0.05**: Moderate smoothing (95% noise) — **recommended default**\n",
    "- **SNR = 0.10**: Light smoothing (90% noise)\n",
    "- **SNR = 0.20**: Minimal smoothing (80% noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract arrays for smoothing\n",
    "y = ics_monthly['ics_hat'].values\n",
    "H = ics_monthly['var_mean'].values\n",
    "\n",
    "# Apply smoothing with different SNR values\n",
    "snr_values = [0.01, 0.05, 0.10, 0.20]\n",
    "results = {}\n",
    "\n",
    "for snr in snr_values:\n",
    "    mu_smooth, se_smooth, Q = smooth_with_fixed_snr(y, H, snr=snr)\n",
    "    results[snr] = {\n",
    "        'mu_smooth': mu_smooth,\n",
    "        'se_smooth': se_smooth,\n",
    "        'Q': Q,\n",
    "        'correlation': np.corrcoef(y, mu_smooth)[0, 1]\n",
    "    }\n",
    "    print(f\"SNR = {snr:.2f}: Q = {Q:.4f}, Correlation with raw = {results[snr]['correlation']:.6f}\")\n",
    "\n",
    "# Add recommended smoothing (SNR = 0.05) to main dataframe\n",
    "ics_monthly['ics_smooth'] = results[0.05]['mu_smooth']\n",
    "ics_monthly['ics_smooth_se'] = results[0.05]['se_smooth']\n",
    "ics_monthly['ics_smooth_lo'] = ics_monthly['ics_smooth'] - 1.96 * ics_monthly['ics_smooth_se']\n",
    "ics_monthly['ics_smooth_hi'] = ics_monthly['ics_smooth'] + 1.96 * ics_monthly['ics_smooth_se']\n",
    "\n",
    "print(\"\\n✓ Smoothing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results\n",
    "\n",
    "### Compare Raw vs Smoothed ICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot uncertainty band\n",
    "ax.fill_between(ics_monthly['date'], \n",
    "                ics_monthly['ics_smooth_lo'],\n",
    "                ics_monthly['ics_smooth_hi'],\n",
    "                alpha=0.3, color='blue', label='95% CI (Smoothed)')\n",
    "\n",
    "# Plot raw and smoothed series\n",
    "ax.plot(ics_monthly['date'], ics_monthly['ics_hat'], \n",
    "        linewidth=1.5, alpha=0.6, color='red', label='Raw ICS (Weighted Mean)')\n",
    "ax.plot(ics_monthly['date'], ics_monthly['ics_smooth'], \n",
    "        linewidth=2.5, color='darkblue', label='Smoothed ICS (Kalman, SNR=0.05)')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=13)\n",
    "ax.set_ylabel('Index of Consumer Sentiment', fontsize=13)\n",
    "ax.set_title('Index of Consumer Sentiment: Raw vs Kalman Smoothed\\n(Variance-Aware with SNR = 0.05)', \n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Different SNR Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot raw data\n",
    "ax.plot(ics_monthly['date'], y, \n",
    "        linewidth=1, alpha=0.4, color='gray', label='Raw ICS', zorder=1)\n",
    "\n",
    "# Plot different smoothing levels\n",
    "colors = ['green', 'blue', 'orange', 'red']\n",
    "for (snr, color) in zip(snr_values, colors):\n",
    "    ax.plot(ics_monthly['date'], results[snr]['mu_smooth'], \n",
    "            linewidth=2, color=color, label=f'Smoothed (SNR={snr:.2f})', zorder=2)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=13)\n",
    "ax.set_ylabel('Index of Consumer Sentiment', fontsize=13)\n",
    "ax.set_title('Comparison of Kalman Smoothing with Different Signal-to-Noise Ratios', \n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  • Lower SNR (green) → More smoothing → Captures long-term trends\")\n",
    "print(\"  • Higher SNR (red) → Less smoothing → Preserves more short-term variation\")\n",
    "print(\"  • SNR = 0.05 (blue) → Recommended default balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoom into Recent Period\n",
    "\n",
    "Let's examine the most recent 3 years to see the smoothing effect more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recent data (last 36 months)\n",
    "recent_data = ics_monthly.tail(36)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Uncertainty band\n",
    "ax.fill_between(recent_data['date'], \n",
    "                recent_data['ics_smooth_lo'],\n",
    "                recent_data['ics_smooth_hi'],\n",
    "                alpha=0.3, color='blue', label='95% CI')\n",
    "\n",
    "# Raw and smoothed\n",
    "ax.plot(recent_data['date'], recent_data['ics_hat'], \n",
    "        'o-', linewidth=1.5, markersize=5, alpha=0.6, color='red', label='Raw ICS')\n",
    "ax.plot(recent_data['date'], recent_data['ics_smooth'], \n",
    "        linewidth=3, color='darkblue', label='Smoothed ICS')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=13)\n",
    "ax.set_ylabel('Index of Consumer Sentiment', fontsize=13)\n",
    "ax.set_title('Recent Period: Raw vs Smoothed ICS (Last 36 Months)', \n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Diagnostics and Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"SMOOTHING DIAGNOSTICS (SNR = 0.05)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Variance reduction\n",
    "var_raw = np.var(ics_monthly['ics_hat'])\n",
    "var_smooth = np.var(ics_monthly['ics_smooth'])\n",
    "var_reduction = (1 - var_smooth / var_raw) * 100\n",
    "\n",
    "print(f\"\\nVariance Reduction:\")\n",
    "print(f\"  Raw variance:      {var_raw:.4f}\")\n",
    "print(f\"  Smoothed variance: {var_smooth:.4f}\")\n",
    "print(f\"  Reduction:         {var_reduction:.2f}%\")\n",
    "\n",
    "# Average uncertainty reduction\n",
    "avg_se_raw = np.mean(ics_monthly['se_mean'])\n",
    "avg_se_smooth = np.mean(ics_monthly['ics_smooth_se'])\n",
    "se_reduction = (1 - avg_se_smooth / avg_se_raw) * 100\n",
    "\n",
    "print(f\"\\nUncertainty Reduction:\")\n",
    "print(f\"  Avg SE (raw):      {avg_se_raw:.4f}\")\n",
    "print(f\"  Avg SE (smoothed): {avg_se_smooth:.4f}\")\n",
    "print(f\"  Reduction:         {se_reduction:.2f}%\")\n",
    "\n",
    "# Correlation\n",
    "correlation = ics_monthly['ics_hat'].corr(ics_monthly['ics_smooth'])\n",
    "print(f\"\\nCorrelation (raw vs smoothed): {correlation:.6f}\")\n",
    "\n",
    "# Process variance\n",
    "print(f\"\\nEstimated Parameters:\")\n",
    "print(f\"  Process variance (Q):        {results[0.05]['Q']:.6f}\")\n",
    "print(f\"  Avg measurement variance (H̄): {np.mean(H):.6f}\")\n",
    "print(f\"  Ratio Q/H̄:                    {results[0.05]['Q'] / np.mean(H):.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results\n",
    "\n",
    "Save the smoothed data for downstream use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final output dataframe\n",
    "output_df = ics_monthly[[\n",
    "    'date', 'n', 'n_eff', 'ics_hat', 'se_mean',\n",
    "    'ics_smooth', 'ics_smooth_se', 'ics_smooth_lo', 'ics_smooth_hi'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "output_df.columns = [\n",
    "    'date', 'n', 'n_eff', 'ics_raw', 'ics_raw_se',\n",
    "    'ics_smooth', 'ics_smooth_se', 'ics_smooth_lo', 'ics_smooth_hi'\n",
    "]\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv('ics_smoothed_output.csv', index=False)\n",
    "print(\"✓ Results saved to 'ics_smoothed_output.csv'\")\n",
    "\n",
    "# Display final results\n",
    "print(f\"\\nFinal output shape: {output_df.shape}\")\n",
    "output_df.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Variance-aware smoothing works**: By incorporating period-specific measurement variance ($H_t$), the smoother automatically adapts to data quality\n",
    "\n",
    "2. **Optimal SNR**: For the ICS data, SNR = 0.05 provides a good balance:\n",
    "   - Removes high-frequency noise from sampling\n",
    "   - Preserves genuine month-to-month changes\n",
    "   - Reduces uncertainty (narrower confidence bands)\n",
    "\n",
    "3. **Interpretability**: The state space model has clear interpretation:\n",
    "   - $Q$ controls how much the true signal can change\n",
    "   - $H_t$ captures sampling uncertainty in each period\n",
    "   - SNR ratio is intuitive for business users\n",
    "\n",
    "### For SaaS Implementation\n",
    "\n",
    "**Recommended user interface:**\n",
    "- Default: SNR = 0.05 (\"Medium smoothing\")\n",
    "- Allow adjustment: 0.01 (\"High\"), 0.10 (\"Low\"), 0.20 (\"Minimal\")\n",
    "- Always show both raw and smoothed with confidence bands\n",
    "- Document method as \"Variance-aware Kalman smoothing\"\n",
    "\n",
    "**Advantages over alternatives:**\n",
    "- ✅ Principled statistical framework\n",
    "- ✅ Automatic adaptation to sample size\n",
    "- ✅ Built-in uncertainty quantification\n",
    "- ✅ Fast computation (no training required)\n",
    "- ✅ Interpretable parameters\n",
    "- ✅ Works with small datasets (<100 periods)\n",
    "\n",
    "**Next steps:**\n",
    "1. Test on other survey KPIs\n",
    "2. Add diagnostics (residual plots, AIC/BIC for model selection)\n",
    "3. Consider seasonal extensions if needed\n",
    "4. Implement in production backend (Python/R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
