# -*- coding: utf-8 -*-
"""Raw vs Smoothed ICS Plot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rnFB39w0QQM1zTBSGrNCeXkMgPt9uR0z
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize
import warnings

# Suppress the UserWarning from minimizing the log-likelihood
warnings.filterwarnings("ignore", message="The Hessian matrix at the final iterate is not positive definite")


# --- Custom Kalman Filter and Smoother Implementation (Local Level) ---

class KalmanFilter:
    """
    Kalman filter for Local Level model (y_t = mu_t + eps_t, mu_t = mu_{t-1} + eta_t)
    Used here to compute the likelihood for optimization.
    """
    def __init__(self, y, H, Q):
        self.y = np.asarray(y)
        self.H = np.asarray(H)
        self.Q = Q
        self.T = len(y)

    def filter(self):
        # Initialize
        mu_pred = np.zeros(self.T)
        P_pred = np.zeros(self.T)
        v = np.zeros(self.T) # Prediction error (innovation)
        F = np.zeros(self.T) # Prediction variance

        # Diffuse initial state (P0)
        first_valid_idx = np.where(np.isfinite(self.y))[0]
        if len(first_valid_idx) > 0:
            mu_pred[0] = self.y[first_valid_idx[0]]
            P_pred[0] = self.H[first_valid_idx[0]] + self.Q
        else:
            mu_pred[0] = 0
            P_pred[0] = 1e6 # Fallback if all data is missing

        # Forward pass
        for t in range(self.T):

            # Prediction and Update only if y[t] is observed
            if np.isfinite(self.y[t]):
                # Prediction step
                F[t] = P_pred[t] + self.H[t]
                v[t] = self.y[t] - mu_pred[t]

                # Update (filter) step
                if F[t] > 1e-10:
                    K = P_pred[t] / F[t]  # Kalman gain
                    mu_filt = mu_pred[t] + K * v[t]
                    P_filt = (1 - K) * P_pred[t]
                else:
                    mu_filt = mu_pred[t]
                    P_filt = P_pred[t]
            else:
                # If observation is missing, filter = prediction
                mu_filt = mu_pred[t]
                P_filt = P_pred[t]

            # Prediction for next step
            if t < self.T - 1:
                mu_pred[t + 1] = mu_filt
                P_pred[t + 1] = P_filt + self.Q

        self.v = v
        self.F = F
        self.P_pred = P_pred
        return v, F

class KalmanSmoother(KalmanFilter):
    """
    Extends KalmanFilter to include the backward pass for smoothing.
    """
    def __init__(self, y, H, Q):
        super().__init__(y, H, Q)
        self.mu_pred = np.zeros(self.T)
        self.mu_filt = np.zeros(self.T)
        self.P_pred = np.zeros(self.T)
        self.P_filt = np.zeros(self.T)

    def filter(self):
        # Full filter implementation to store all intermediate states

        first_valid_idx = np.where(np.isfinite(self.y))[0]
        if len(first_valid_idx) > 0:
            self.mu_pred[0] = self.y[first_valid_idx[0]]
            self.P_pred[0] = self.H[first_valid_idx[0]] + self.Q
        else:
            self.mu_pred[0] = 0
            self.P_pred[0] = 1e6

        for t in range(self.T):
            if np.isfinite(self.y[t]):
                F = self.P_pred[t] + self.H[t]

                if F > 1e-10:
                    K = self.P_pred[t] / F
                    self.mu_filt[t] = self.mu_pred[t] + K * (self.y[t] - self.mu_pred[t])
                    self.P_filt[t] = (1 - K) * self.P_pred[t]
                else:
                    self.mu_filt[t] = self.mu_pred[t]
                    self.P_filt[t] = self.P_pred[t]
            else:
                self.mu_filt[t] = self.mu_pred[t]
                self.P_filt[t] = self.P_pred[t]

            if t < self.T - 1:
                self.mu_pred[t + 1] = self.mu_filt[t]
                self.P_pred[t + 1] = self.P_filt[t] + self.Q

    def smooth(self):
        """Run Kalman smoother (backward pass)"""
        self.mu_smooth = np.zeros(self.T)
        self.P_smooth = np.zeros(self.T)

        self.mu_smooth[-1] = self.mu_filt[-1]
        self.P_smooth[-1] = self.P_filt[-1]

        for t in range(self.T - 2, -1, -1):
            J = self.P_filt[t] / max(self.P_pred[t + 1], 1e-10)

            self.mu_smooth[t] = self.mu_filt[t] + J * (self.mu_smooth[t + 1] - self.mu_pred[t + 1])
            self.P_smooth[t] = self.P_filt[t] + J**2 * (self.P_smooth[t + 1] - self.P_pred[t + 1])

    def run(self):
        """Run both filter and smoother"""
        self.filter()
        self.smooth()
        return self.mu_smooth, np.sqrt(self.P_smooth)

def neg_log_likelihood(params, y, H):
    """Calculates the negative log-likelihood for optimization (Replicates R's log-likelihood)"""
    log_Q = params[0]
    Q = np.exp(log_Q)

    kf = KalmanFilter(y, H, Q)
    v, F = kf.filter()

    finite_v_F = (np.isfinite(v) & (F > 1e-10))

    if np.sum(finite_v_F) == 0:
        return 1e10

    log_likelihood_sum = -0.5 * np.sum(np.log(F[finite_v_F]) + (v[finite_v_F]**2 / F[finite_v_F]) + np.log(2 * np.pi))

    return -log_likelihood_sum

def optimize_Q_and_smooth(y, H):
    """
    Finds the optimal Q via ML and runs the Kalman Smoother. (Replicates R's fitSSM)
    """
    y_var = np.nanvar(y)
    theta0 = np.log(y_var * 0.05) # R's initial value heuristic

    res = minimize(neg_log_likelihood, x0=[theta0], args=(y, H), method='BFGS')

    log_Q_ml = res.x[0]
    Q_ml = np.exp(log_Q_ml)

    # Run the Kalman Smoother with the optimized Q
    smoother = KalmanSmoother(y, H, Q_ml)
    mu_smooth, se_smooth = smoother.run()

    return mu_smooth, se_smooth, Q_ml, res.success

# --- Full Data Pipeline and Plotting ---

def run_full_analysis_and_plot():
    """
    Loads data from GitHub, performs weighted mean/variance calculation,
    runs Kalman Smoothing, and generates the final plot.
    """
    # =========================================================================
    # *** DATA LOADING FROM GITHUB (Replicating R's read_csv calls) ***
    # =========================================================================

    URL_ICS_MONTH_ID = "https://raw.githubusercontent.com/oysteinm/smood/main/AAk7MRJC.csv"

    print("Loading ICS individual response data from GitHub...")
    try:
        ics_data_id = pd.read_csv(URL_ICS_MONTH_ID,
                                  dtype={'id': str},
                                  parse_dates=['YYYYMM'],
                                  date_format="%Y%m")
    except Exception as e:
        print(f"Error loading data from GitHub: {e}")
        # Cannot proceed without data
        return

    # Clean column names (R's janitor::clean_names) and rename for use
    ics_data_id.columns = ics_data_id.columns.str.lower()
    ics_data_id = ics_data_id.rename(columns={'yyyymm': 'date'})

    # Filter for finite and positive weights (R's filter)
    ics_data_id = ics_data_id[
        ics_data_id['ics'].isfinite() &
        ics_data_id['wt'].isfinite() &
        (ics_data_id['wt'] > 0)
    ]

    # --- R step 1: Calculate Monthly Weighted Mean and Variance (ics_monthly) ---
    print("Calculating weighted mean and measurement variance...")

    # Aggregation requires custom application for weighted statistics
    def weighted_stats(group):
        wt = group['wt'].values
        ics = group['ics'].values

        w_sum = np.sum(wt)
        w2_sum = np.sum(wt**2)
        n = len(group)

        if w_sum == 0 or denom <= 1e-8:
            return pd.Series([n, 0, 0, np.nan, np.nan, np.nan, np.nan, np.nan],
                             index=['n', 'w_sum', 'w2_sum', 'ics_hat', 'n_eff', 's2_w', 'var_mean', 'se_mean'])

        ics_hat = np.sum(wt * ics) / w_sum
        n_eff = w_sum**2 / w2_sum

        # Weighted Bessel denominator: pmax(w_sum - (w2_sum / w_sum), 1e-8)
        denom = w_sum - (w2_sum / w_sum)
        denom = np.maximum(denom, 1e-8)

        s2_w = np.sum(wt * (ics - ics_hat)**2) / denom # weighted variance of units
        var_mean = s2_w / np.maximum(n_eff, 1e-8)       # Var(monthly mean)
        se_mean = np.sqrt(np.maximum(var_mean, 1e-12))

        return pd.Series([n, w_sum, w2_sum, ics_hat, n_eff, s2_w, var_mean, se_mean],
                         index=['n', 'w_sum', 'w2_sum', 'ics_hat', 'n_eff', 's2_w', 'var_mean', 'se_mean'])

    ics_monthly = ics_data_id.groupby('date').apply(weighted_stats).reset_index()
    ics_monthly = ics_monthly.sort_values('date')

    # --- R step 2: Guards and Preparation for Smoothing (y, H_vec) ---
    y = ics_monthly['ics_hat'].values
    H_vec = ics_monthly['var_mean'].values

    # R's guards: handle NA/Inf or nonpositive H_vec
    good = np.isfinite(H_vec) & (H_vec > 0)

    # Calculate fallback based on good data
    if np.sum(good) > 0:
        fallback = np.median(H_vec[good])
    else:
        fallback = np.nanvar(y) * 0.05

    # Handle case where fallback is invalid
    if not np.isfinite(fallback) or fallback <= 0:
        fallback = 1.0 # Default safe fallback

    H_vec[~good] = fallback

    # Gentle floor (R's pmax)
    floor_val = np.quantile(H_vec, 0.05) * 0.1
    if not np.isfinite(floor_val) or floor_val <= 0: floor_val = fallback * 0.1
    H_vec = np.maximum(H_vec, floor_val)

    # --- R step 3: Optimize Q and Run Kalman Smoothing (fitSSM, KFS) ---
    print("Starting Maximum Likelihood estimation of State Variance (Q)...")
    ics_sm, ics_sm_se, Q_ml, success = optimize_Q_and_smooth(y, H_vec)
    print(f"Optimization {'Successful' if success else 'Failed'}. ML Q: {Q_ml:.4f}")

    # --- R step 4: Final DF Creation (ics_out) ---
    ics_out = ics_monthly.copy()
    ics_out['ics_raw'] = ics_out['ics_hat']
    ics_out['ics_smooth'] = ics_sm
    ics_out['ics_smooth_se'] = ics_sm_se
    ics_out['ics_smooth_lo'] = ics_sm - 1.96 * ics_sm_se
    ics_out['ics_smooth_hi'] = ics_sm + 1.96 * ics_sm_se

    # =========================================================================
    # --- Plotting Logic (Replicating the R ggplot code) ---
    # =========================================================================

    plt.style.use('seaborn-v0_8-darkgrid')
    sns.set_palette("deep")

    fig, ax = plt.subplots(figsize=(14, 7))

    # geom_ribbon (95% CI)
    ax.fill_between(ics_out['date'],
                    ics_out['ics_smooth_lo'],
                    ics_out['ics_smooth_hi'],
                    alpha=0.5, color='lightblue', label='95% CI (Smoothed)', zorder=1)

    # Raw ICS line (ics_hat)
    ax.plot(ics_out['date'], ics_out['ics_raw'],
            linewidth=1, color='red', label='Raw ICS', zorder=2)

    # Smoothed ICS line (ics_sm)
    ax.plot(ics_out['date'], ics_out['ics_smooth'],
            linewidth=1, color='blue', label='Smoothed ICS', zorder=3)

    # Titles and Labels
    ax.set_title('Index of Consumer Sentiment: Raw vs Smoothed', fontsize=16, fontweight='bold')
    ax.set_ylabel('Index of Consumer Sentiment', fontsize=12)
    ax.set_xlabel('Date', fontsize=12)

    ax.grid(True, alpha=0.3)

    # Custom Legend (to include the ribbon patch)
    from matplotlib.patches import Patch
    raw_line = plt.Line2D([0], [0], color='red', linewidth=1)
    smooth_line = plt.Line2D([0], [0], color='blue', linewidth=1)
    ribbon_patch = Patch(color='lightblue', alpha=0.5)

    ax.legend([smooth_line, raw_line, ribbon_patch],
              ['Smoothed ICS', 'Raw ICS', '95% CI (Smoothed)'],
              title='Legend', loc='best', fontsize=10, title_fontsize=11)

    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    run_full_analysis_and_plot()