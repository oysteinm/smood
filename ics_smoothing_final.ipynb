{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variance-Aware Smoothing for Survey KPIs\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates a principled approach to smoothing time series survey data that accounts for varying sample sizes and measurement uncertainty across time periods.\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Weekly or monthly survey KPIs exhibit noise because each period has finite (and often varying) sample size $n_t$. This sampling error:\n",
        "- Adds variance without adding information\n",
        "- Creates noisy dashboards and unstable trends\n",
        "- Makes period-to-period comparisons difficult\n",
        "\n",
        "**Key insight:** Not all observations are equally reliable. A month with $n=50$ respondents should be smoothed more than a month with $n=500$.\n",
        "\n",
        "### Our Approach\n",
        "\n",
        "We use **Kalman smoothing** with period-specific measurement variance to:\n",
        "1. Estimate measurement uncertainty for each period based on effective sample size\n",
        "2. Apply optimal smoothing that adapts to data quality\n",
        "3. Provide uncertainty bands for the smoothed estimates\n",
        "\n",
        "### Mathematical Framework\n",
        "\n",
        "#### Step 1: Weighted Mean and Measurement Variance\n",
        "\n",
        "For each time period $t$, from individual microdata $(y_{it}, w_{it})$:\n",
        "\n",
        "**Weighted mean:**\n",
        "$$\\hat{y}_t = \\frac{\\sum_i w_{it} y_{it}}{\\sum_i w_{it}}$$\n",
        "\n",
        "**Effective sample size** (accounts for unequal weights):\n",
        "$$n_{\\text{eff},t} = \\frac{(\\sum_i w_{it})^2}{\\sum_i w_{it}^2}$$\n",
        "\n",
        "**Weighted sample variance** (Bessel-corrected):\n",
        "$$s_{w,t}^2 = \\frac{\\sum_i w_{it}(y_{it} - \\hat{y}_t)^2}{\\sum_i w_{it} - \\frac{\\sum_i w_{it}^2}{\\sum_i w_{it}}}$$\n",
        "\n",
        "**Measurement variance of the mean:**\n",
        "$$H_t = \\text{Var}(\\hat{y}_t) \\approx \\frac{s_{w,t}^2}{n_{\\text{eff},t}}$$\n",
        "\n",
        "This gives us period-specific uncertainty: small $n_{\\text{eff},t}$ → large $H_t$ → more smoothing.\n",
        "\n",
        "#### Step 2: State Space Model (Kalman Filter/Smoother)\n",
        "\n",
        "We model the true KPI $\\mu_t$ as a slowly evolving local level:\n",
        "\n",
        "**Observation equation:**\n",
        "$$y_t = \\mu_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, H_t)$$\n",
        "\n",
        "**State equation:**\n",
        "$$\\mu_t = \\mu_{t-1} + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, Q)$$\n",
        "\n",
        "Where:\n",
        "- $H_t$: measurement variance (from survey sampling, varies by period)\n",
        "- $Q$: process variance (how much the true signal can change month-to-month)\n",
        "\n",
        "#### Step 3: Maximum Likelihood Estimation of Q\n",
        "\n",
        "Rather than fixing the signal-to-noise ratio, we estimate $Q$ using maximum likelihood estimation (MLE). This approach:\n",
        "- Uses the observed data to find the optimal process variance\n",
        "- Automatically balances smoothness vs fit to the data\n",
        "- Provides a more data-driven solution\n",
        "\n",
        "The MLE approach maximizes the log-likelihood of the observed data under the state space model, finding the value of $Q$ that best explains the patterns in the data given the known measurement variances $H_t$.\n",
        "\n",
        "### Data: University of Michigan Index of Consumer Sentiment\n",
        "\n",
        "We'll use the monthly ICS data which includes:\n",
        "- Individual response microdata with survey weights\n",
        "- Published aggregate index values\n",
        "\n",
        "This is an ideal test case because we can:\n",
        "1. Calculate weighted means ourselves\n",
        "2. Estimate measurement variance from effective sample size\n",
        "3. Apply our smoothing procedure\n",
        "4. Compare to published values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data from GitHub\n",
        "\n",
        "We'll load two datasets:\n",
        "1. **Individual microdata** (`AAk7MRJC.csv`): Individual survey responses with weights\n",
        "2. **Published aggregate** (`scaum-479.csv`): Official monthly ICS values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# URLs (GitHub raw content)\n",
        "URL_PUBLISHED  = \"https://raw.githubusercontent.com/oysteinm/smood/refs/heads/main/scaum-479.csv\"\n",
        "URL_INDIVIDUAL = \"https://raw.githubusercontent.com/oysteinm/smood/refs/heads/main/AAk7MRJC.csv\"\n",
        "\n",
        "# Load published aggregate data\n",
        "ics_month = pd.read_csv(URL_PUBLISHED)\n",
        "ics_month[\"date\"] = pd.to_datetime(ics_month[\"yyyymm\"], format=\"%Y%m\", errors=\"coerce\")\n",
        "ics_month[\"ics_all\"] = pd.to_numeric(ics_month[\"ics_all\"], errors=\"coerce\")\n",
        "ics_data = ics_month[[\"date\", \"ics_all\"]].rename(columns={\"ics_all\": \"ics_published\"})\n",
        "\n",
        "print(f\"Published data loaded: {len(ics_data)} months\")\n",
        "print(f\"Date range: {ics_data['date'].min()} to {ics_data['date'].max()}\")\n",
        "ics_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load individual microdata\n",
        "ics_id = pd.read_csv(URL_INDIVIDUAL)\n",
        "ics_id.columns = (\n",
        "    ics_id.columns.str.strip()\n",
        "    .str.lower()\n",
        "    .str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
        ")\n",
        "\n",
        "# Expect YYYYMM, id, ics, wt\n",
        "ics_id[\"date\"] = pd.to_datetime(ics_id[\"yyyymm\"], format=\"%Y%m\", errors=\"coerce\")\n",
        "ics_id[\"ics\"] = pd.to_numeric(ics_id[\"ics\"], errors=\"coerce\")\n",
        "ics_id[\"wt\"]  = pd.to_numeric(ics_id[\"wt\"], errors=\"coerce\")\n",
        "\n",
        "print(f\"\\nIndividual microdata loaded: {len(ics_id)} responses\")\n",
        "print(f\"Unique months: {ics_id['date'].nunique()}\")\n",
        "ics_id.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Calculate Monthly Weighted Means and Measurement Variance\n",
        "\n",
        "For each month, we compute:\n",
        "- Weighted mean of ICS\n",
        "- Effective sample size (accounting for unequal weights)\n",
        "- Weighted variance\n",
        "- Measurement variance (standard error of the mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def monthly_stats(df):\n",
        "    \"\"\"Calculate weighted monthly statistics for survey data.\"\"\"\n",
        "    d = df.loc[np.isfinite(df[\"ics\"]) & np.isfinite(df[\"wt\"]) & (df[\"wt\"] > 0)].copy()\n",
        "\n",
        "    # Group-wise weighted mean, n_eff, var\n",
        "    out = []\n",
        "    for date, g in d.groupby(\"date\"):\n",
        "        w = g[\"wt\"].to_numpy()\n",
        "        x = g[\"ics\"].to_numpy()\n",
        "        n = len(x)\n",
        "        w_sum  = w.sum()\n",
        "        w2_sum = (w**2).sum()\n",
        "        \n",
        "        # Effective sample size\n",
        "        n_eff  = (w_sum**2) / max(w2_sum, 1e-12)\n",
        "        \n",
        "        # Weighted mean\n",
        "        xhat   = (w * x).sum() / w_sum\n",
        "        \n",
        "        # Bessel-corrected weighted variance\n",
        "        denom  = max(w_sum - (w2_sum / w_sum), 1e-8)\n",
        "        s2_w   = (w * (x - xhat)**2).sum() / denom\n",
        "        \n",
        "        # Variance of the mean and standard error\n",
        "        var_mean = s2_w / max(n_eff, 1e-8)\n",
        "        se_mean  = np.sqrt(max(var_mean, 1e-12))\n",
        "        \n",
        "        out.append((date, n, n_eff, w_sum, w2_sum, xhat, s2_w, var_mean, se_mean))\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        out, columns=[\"date\", \"n\", \"n_eff\", \"w_sum\", \"w2_sum\",\n",
        "                      \"ics_hat\", \"s2_w\", \"var_mean\", \"se_mean\"]\n",
        "    ).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Calculate monthly statistics\n",
        "ics_monthly = monthly_stats(ics_id)\n",
        "\n",
        "print(f\"Monthly statistics calculated for {len(ics_monthly)} months\")\n",
        "print(f\"\\nSample size statistics:\")\n",
        "print(f\"  Median n_eff: {ics_monthly['n_eff'].median():.1f}\")\n",
        "print(f\"  Min n_eff: {ics_monthly['n_eff'].min():.1f}\")\n",
        "print(f\"  Max n_eff: {ics_monthly['n_eff'].max():.1f}\")\n",
        "\n",
        "ics_monthly.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Sample Size and Measurement Variance Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# Top panel: Measurement variance (H_t)\n",
        "ax1.plot(ics_monthly['date'], ics_monthly['var_mean'],\n",
        "         linewidth=2, color='steelblue', label='Measurement Variance ($H_t$)')\n",
        "ax1.set_ylabel('Variance', fontsize=12)\n",
        "ax1.set_title('Measurement Variance Over Time', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Bottom panel: Effective sample size over time\n",
        "ax2.plot(ics_monthly['date'], ics_monthly['n_eff'],\n",
        "         linewidth=2, color='green', label='Effective Sample Size')\n",
        "ax2.axhline(y=ics_monthly['n_eff'].median(), color='red',\n",
        "            linestyle='--', label=f'Median = {ics_monthly[\"n_eff\"].median():.0f}')\n",
        "ax2.set_xlabel('Date', fontsize=12)\n",
        "ax2.set_ylabel('Effective n', fontsize=12)\n",
        "ax2.set_title('Effective Sample Size Over Time', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Kalman Smoothing with MLE-Estimated Process Variance\n",
        "\n",
        "Now we apply the Kalman filter/smoother using the **local level model** with:\n",
        "- Time-varying measurement variance $H_t$ (from our calculations above)\n",
        "- Process variance $Q$ estimated via maximum likelihood estimation (MLE)\n",
        "\n",
        "### Prepare Measurement Variance H_t\n",
        "\n",
        "We need to ensure all measurement variances are positive and handle any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract measurement variance and observations\n",
        "H = ics_monthly[\"var_mean\"].to_numpy().astype(float)\n",
        "y = ics_monthly[\"ics_hat\"].to_numpy().astype(float)\n",
        "\n",
        "# Guard for H_t: ensure all values are positive and finite\n",
        "good = np.isfinite(H) & (H > 0)\n",
        "fallback = np.nanmedian(H[good]) if np.any(good) else (np.nanvar(y) * 0.05)\n",
        "if not np.isfinite(fallback):\n",
        "    fallback = np.nanvar(y) * 0.05\n",
        "H[~good] = fallback\n",
        "\n",
        "# Apply a floor to prevent extremely small variances\n",
        "q05 = np.nanquantile(H, 0.05) if np.isfinite(np.nanquantile(H, 0.05)) else np.nan\n",
        "floor_val = 0.1 * q05 if (np.isfinite(q05) and q05 > 0) else 0.1 * fallback\n",
        "H = np.maximum(H, floor_val)\n",
        "\n",
        "print(f\"Measurement variance (H_t) statistics:\")\n",
        "print(f\"  Mean: {H.mean():.6f}\")\n",
        "print(f\"  Median: {np.median(H):.6f}\")\n",
        "print(f\"  Min: {H.min():.6f}\")\n",
        "print(f\"  Max: {H.max():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kalman Filter Log-Likelihood\n",
        "\n",
        "We implement the Kalman filter to compute the log-likelihood of the data given parameters $Q$ and $H_t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kalman_loglike(y, H, Q, P0=1e6):\n",
        "    \"\"\"Local level model log-likelihood (handles missing y).\"\"\"\n",
        "    T = len(y)\n",
        "    mu_pred = 0.0\n",
        "    P_pred = P0 + Q\n",
        "    ll = 0.0\n",
        "    \n",
        "    for t in range(T):\n",
        "        yt = y[t]\n",
        "        Ht = H[t]\n",
        "        \n",
        "        if np.isfinite(yt):\n",
        "            # Innovation variance\n",
        "            S = P_pred + Ht\n",
        "            S = np.maximum(S, 1e-8)\n",
        "            \n",
        "            # Innovation\n",
        "            v = yt - mu_pred\n",
        "            \n",
        "            # Log-likelihood contribution\n",
        "            ll -= 0.5 * (np.log(2*np.pi) + np.log(S) + (v*v)/S)\n",
        "            \n",
        "            # Kalman gain and update\n",
        "            K = P_pred / S\n",
        "            mu_filt = mu_pred + K*v\n",
        "            P_filt = (1 - K) * P_pred\n",
        "        else:\n",
        "            # Skip update for missing observations\n",
        "            mu_filt = mu_pred\n",
        "            P_filt = P_pred\n",
        "        \n",
        "        # Predict next time step\n",
        "        mu_pred = mu_filt\n",
        "        P_pred  = P_filt + Q\n",
        "    \n",
        "    return ll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Maximum Likelihood Estimation of Q\n",
        "\n",
        "We use numerical optimization to find the value of $Q$ that maximizes the log-likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_Q_mle(y, H, q0=None):\n",
        "    \"\"\"Estimate Q via maximum likelihood.\"\"\"\n",
        "    if q0 is None:\n",
        "        q0 = np.nanmean(H) * 0.05\n",
        "        if not np.isfinite(q0) or q0 <= 0:\n",
        "            q0 = np.nanvar(y) * 0.05\n",
        "    \n",
        "    # Optimize in log-space to ensure Q > 0\n",
        "    def nll(logQ):\n",
        "        Q = np.exp(logQ)\n",
        "        return -kalman_loglike(y, H, Q)\n",
        "    \n",
        "    res = minimize(nll, x0=np.log(q0), method=\"L-BFGS-B\")\n",
        "    return float(np.exp(res.x)), res\n",
        "\n",
        "# Estimate Q\n",
        "Q_hat, res = estimate_Q_mle(y, H)\n",
        "print(f\"\\nEstimated process variance (Q): {Q_hat:.6g}\")\n",
        "print(f\"Optimization converged: {res.success}\")\n",
        "print(f\"Log-likelihood: {-res.fun:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kalman Filter (Forward Pass)\n",
        "\n",
        "Store the filtered estimates at each time point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kalman_filter_store(y, H, Q, P0=1e6):\n",
        "    \"\"\"Run Kalman filter and store all intermediate values.\"\"\"\n",
        "    T = len(y)\n",
        "    mu_pred = np.zeros(T)\n",
        "    P_pred  = np.zeros(T)\n",
        "    mu_filt = np.zeros(T)\n",
        "    P_filt  = np.zeros(T)\n",
        "\n",
        "    # Initial prediction\n",
        "    mu_pred[0] = 0.0\n",
        "    P_pred[0]  = P0 + Q\n",
        "\n",
        "    # Filter first observation\n",
        "    if np.isfinite(y[0]):\n",
        "        S = P_pred[0] + H[0]\n",
        "        S = max(S, 1e-8)\n",
        "        K = P_pred[0] / S\n",
        "        v = y[0] - mu_pred[0]\n",
        "        mu_filt[0] = mu_pred[0] + K*v\n",
        "        P_filt[0]  = (1 - K) * P_pred[0]\n",
        "    else:\n",
        "        mu_filt[0] = mu_pred[0]\n",
        "        P_filt[0]  = P_pred[0]\n",
        "\n",
        "    # Filter remaining observations\n",
        "    for t in range(1, T):\n",
        "        mu_pred[t] = mu_filt[t-1]\n",
        "        P_pred[t]  = P_filt[t-1] + Q\n",
        "        \n",
        "        if np.isfinite(y[t]):\n",
        "            S = P_pred[t] + H[t]\n",
        "            S = max(S, 1e-8)\n",
        "            K = P_pred[t] / S\n",
        "            v = y[t] - mu_pred[t]\n",
        "            mu_filt[t] = mu_pred[t] + K*v\n",
        "            P_filt[t]  = (1 - K) * P_pred[t]\n",
        "        else:\n",
        "            mu_filt[t] = mu_pred[t]\n",
        "            P_filt[t]  = P_pred[t]\n",
        "    \n",
        "    return mu_pred, P_pred, mu_filt, P_filt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RTS Smoother (Backward Pass)\n",
        "\n",
        "Apply the Rauch-Tung-Striebel (RTS) smoother to get smoothed estimates using all available data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rts_smoother(mu_pred, P_pred, mu_filt, P_filt, Q):\n",
        "    \"\"\"Apply RTS smoother backward pass.\"\"\"\n",
        "    T = len(mu_filt)\n",
        "    mu_smooth = np.zeros(T)\n",
        "    P_smooth  = np.zeros(T)\n",
        "    \n",
        "    # Initialize with filtered values at last time point\n",
        "    mu_smooth[-1] = mu_filt[-1]\n",
        "    P_smooth[-1]  = P_filt[-1]\n",
        "    \n",
        "    # Backward pass\n",
        "    for t in range(T-2, -1, -1):\n",
        "        denom = P_pred[t+1]\n",
        "        denom = np.maximum(denom, 1e-8)\n",
        "        J = P_filt[t] / denom\n",
        "        mu_smooth[t] = mu_filt[t] + J*(mu_smooth[t+1] - mu_pred[t+1])\n",
        "        P_smooth[t]  = P_filt[t] + (J**2)*(P_smooth[t+1] - P_pred[t+1])\n",
        "    \n",
        "    return mu_smooth, P_smooth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Kalman Filter and Smoother"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run filter\n",
        "mu_pred, P_pred, mu_filt, P_filt = kalman_filter_store(y, H, Q_hat)\n",
        "\n",
        "# Run smoother\n",
        "mu_smooth, P_smooth = rts_smoother(mu_pred, P_pred, mu_filt, P_filt, Q_hat)\n",
        "\n",
        "# Calculate standard errors\n",
        "se_smooth = np.sqrt(np.maximum(P_smooth, 0.0))\n",
        "\n",
        "print(\"Kalman filter and smoother completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Results and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine results\n",
        "out = ics_monthly.copy()\n",
        "out[\"ics_sm\"]    = mu_smooth\n",
        "out[\"ics_sm_lo\"] = mu_smooth - 1.96 * se_smooth\n",
        "out[\"ics_sm_hi\"] = mu_smooth + 1.96 * se_smooth\n",
        "\n",
        "# Join with published index\n",
        "ics_cmp = out.merge(ics_data, on=\"date\", how=\"left\")\n",
        "\n",
        "print(\"\\nFinal dataset with smoothed values:\")\n",
        "print(f\"  Total months: {len(ics_cmp)}\")\n",
        "print(f\"  Date range: {ics_cmp['date'].min()} to {ics_cmp['date'].max()}\")\n",
        "\n",
        "ics_cmp[[\"date\", \"ics_hat\", \"ics_sm\", \"ics_sm_lo\", \"ics_sm_hi\", \"ics_published\"]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Raw vs Smoothed ICS with 95% Confidence Band"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# 95% confidence band\n",
        "plt.fill_between(ics_cmp[\"date\"], ics_cmp[\"ics_sm_lo\"], ics_cmp[\"ics_sm_hi\"],\n",
        "                 color=\"lightblue\", alpha=0.4, label=\"Smoothed 95% band\")\n",
        "\n",
        "# Raw ICS\n",
        "plt.plot(ics_cmp[\"date\"], ics_cmp[\"ics_hat\"], \n",
        "         color=\"red\", label=\"Raw ICS\", linewidth=1.2, alpha=0.7)\n",
        "\n",
        "# Smoothed ICS\n",
        "plt.plot(ics_cmp[\"date\"], ics_cmp[\"ics_sm\"], \n",
        "         color=\"blue\", label=\"Smoothed ICS\", linewidth=1.5)\n",
        "\n",
        "plt.title(\"Index of Consumer Sentiment: Raw vs Smoothed\", fontsize=14, fontweight='bold')\n",
        "plt.ylabel(\"Index of Consumer Sentiment\", fontsize=12)\n",
        "plt.xlabel(\"Date\", fontsize=12)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(alpha=0.25)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Diagnostics and Validation\n",
        "\n",
        "Let's examine how well the smoothed series tracks the raw observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlations\n",
        "valid = np.isfinite(ics_cmp[\"ics_hat\"]) & np.isfinite(ics_cmp[\"ics_sm\"])\n",
        "\n",
        "corr_raw_sm = np.corrcoef(ics_cmp.loc[valid, \"ics_hat\"], \n",
        "                          ics_cmp.loc[valid, \"ics_sm\"])[0,1]\n",
        "\n",
        "corr_round1 = np.corrcoef(np.round(ics_cmp.loc[valid, \"ics_hat\"], 1),\n",
        "                          np.round(ics_cmp.loc[valid, \"ics_sm\"], 1))[0,1]\n",
        "\n",
        "print(\"Diagnostics:\")\n",
        "print(f\"  Correlation (raw vs smoothed): {corr_raw_sm:.6f}\")\n",
        "print(f\"  Correlation (1-decimal rounding): {corr_round1:.6f}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(f\"  Raw ICS mean: {ics_cmp['ics_hat'].mean():.2f}\")\n",
        "print(f\"  Raw ICS std: {ics_cmp['ics_hat'].std():.2f}\")\n",
        "print(f\"  Smoothed ICS mean: {ics_cmp['ics_sm'].mean():.2f}\")\n",
        "print(f\"  Smoothed ICS std: {ics_cmp['ics_sm'].std():.2f}\")\n",
        "print(f\"  Noise reduction: {(1 - ics_cmp['ics_sm'].std()/ics_cmp['ics_hat'].std())*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare with Published Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with published index if available\n",
        "valid_pub = np.isfinite(ics_cmp[\"ics_published\"]) & valid\n",
        "\n",
        "if valid_pub.sum() > 0:\n",
        "    corr_smooth_pub = np.corrcoef(ics_cmp.loc[valid_pub, \"ics_sm\"],\n",
        "                                  ics_cmp.loc[valid_pub, \"ics_published\"])[0,1]\n",
        "    \n",
        "    print(f\"\\nCorrelation with published index: {corr_smooth_pub:.6f}\")\n",
        "    \n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(ics_cmp.loc[valid_pub, \"date\"], \n",
        "             ics_cmp.loc[valid_pub, \"ics_sm\"],\n",
        "             label=\"Our Smoothed ICS\", linewidth=1.5)\n",
        "    plt.plot(ics_cmp.loc[valid_pub, \"date\"], \n",
        "             ics_cmp.loc[valid_pub, \"ics_published\"],\n",
        "             label=\"Published ICS\", linewidth=1.5, linestyle='--', alpha=0.7)\n",
        "    plt.title(\"Smoothed vs Published Index\", fontsize=14, fontweight='bold')\n",
        "    plt.ylabel(\"Index of Consumer Sentiment\", fontsize=12)\n",
        "    plt.xlabel(\"Date\", fontsize=12)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nNo published index data available for comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated variance-aware Kalman smoothing for survey data:\n",
        "\n",
        "1. **Computed measurement variance** from weighted microdata accounting for effective sample size\n",
        "2. **Estimated process variance Q** using maximum likelihood estimation\n",
        "3. **Applied Kalman filter and RTS smoother** to obtain optimal smoothed estimates\n",
        "4. **Provided uncertainty quantification** through 95% confidence bands\n",
        "\n",
        "The approach automatically adapts smoothing intensity based on data quality, providing more smoothing when sample sizes are small and less smoothing when sample sizes are large."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
